{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuCnhG70yvwI",
    "outputId": "bb496956-14d5-4e2c-b8d7-a0ebeb226148"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5qkuTZny6P-",
    "outputId": "a9645f62-af3d-4c9c-9ac9-ffb41494f361"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (0.10.15)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.25.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJBSqGTrzEeD"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8pvO91ozNDA"
   },
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5JYzL8TzSG8"
   },
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    image.flags.writeable = False  \n",
    "    results = model.process(image)  \n",
    "    image.flags.writeable = True  \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  \n",
    "    return image, results\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    # pose landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    # left hand landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    # right hand landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up6HZDaJzWXQ"
   },
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):'\n",
    "    # pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             )\n",
    "    # right hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHqqEy6rza-3"
   },
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose,lh, rh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uWazI2Gzfm-",
    "outputId": "6f6465ad-bac0-4d7a-b646-be80e3f4397d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
      "Frame extraction complete. All frames saved to /content/path_to_save_extracted_frames.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install opencv-python\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "VIDEO_FOLDER_PATH = '/content/drive/MyDrive/train' \n",
    "\n",
    "\n",
    "FRAME_OUTPUT_PATH = '/content/path_to_save_extracted_frames'  \n",
    "\n",
    "\n",
    "os.makedirs(FRAME_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "for video_file in os.listdir(VIDEO_FOLDER_PATH):\n",
    "    video_path = os.path.join(VIDEO_FOLDER_PATH, video_file)\n",
    "\n",
    "    if video_file.endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "\n",
    "       \n",
    "        video_name = os.path.splitext(video_file)[0]  \n",
    "        video_output_dir = os.path.join(FRAME_OUTPUT_PATH, video_name)\n",
    "        os.makedirs(video_output_dir, exist_ok=True)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "       \n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_file}.\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        frame_idx = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()  \n",
    "\n",
    "            if not ret:\n",
    "                break  \n",
    "\n",
    "            \n",
    "            frame_filename = f\"{video_name}_frame{frame_idx:04d}.jpg\"  \n",
    "            frame_output_path = os.path.join(video_output_dir, frame_filename)\n",
    "\n",
    "            \n",
    "            cv2.imwrite(frame_output_path, frame)\n",
    "\n",
    "            frame_idx += 1  \n",
    "\n",
    "       \n",
    "        cap.release()\n",
    "\n",
    "print(f\"Frame extraction complete. All frames saved to {FRAME_OUTPUT_PATH}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjcojeMh7bEh",
    "outputId": "f42f218d-123d-4148-dd6d-14e7af91fbb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing action: absent, Available frames: 372, Sequences to extract: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing action: baby, Available frames: 312, Sequences to extract: 10\n",
      "Processing action: bacteria, Available frames: 474, Sequences to extract: 15\n",
      "Processing action: cabbage, Available frames: 372, Sequences to extract: 12\n",
      "Processing action: call, Available frames: 282, Sequences to extract: 9\n",
      "Processing action: dark, Available frames: 402, Sequences to extract: 13\n",
      "Processing action: face, Available frames: 276, Sequences to extract: 9\n",
      "Processing action: fall, Available frames: 282, Sequences to extract: 9\n",
      "Data shape: (89, 180, 258)\n",
      "Labels shape: (89, 8)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "DATA_PATH = '/content/path_to_save_extracted_frames'\n",
    "actions = np.array([\"absent\", \"baby\", \"bacteria\", \"cabbage\", \"call\", \"dark\", \"face\", \"fall\"]) \n",
    "\n",
    "sequence_length = 30\n",
    "label_map = {label: num for num, label in enumerate(actions)}\n",
    "\n",
    "\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "sequences, labels = [], []\n",
    "\n",
    "\n",
    "def adjust_brightness(image, brightness=30):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v.astype(np.int32) + brightness, 0, 255).astype(np.uint8)\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def augment_image(image):\n",
    "    aug_images = []\n",
    "\n",
    "   \n",
    "    aug_images.append(image)\n",
    "\n",
    "   \n",
    "    aug_images.append(cv2.flip(image, 1))\n",
    "\n",
    "    \n",
    "    aug_images.append(adjust_brightness(image, 30))\n",
    "    aug_images.append(adjust_brightness(image, -30))\n",
    "\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "\n",
    "    \n",
    "    M = cv2.getRotationMatrix2D(center, 10, 1.0)\n",
    "    aug_images.append(cv2.warpAffine(image, M, (w, h)))\n",
    "\n",
    "    M = cv2.getRotationMatrix2D(center, -10, 1.0)\n",
    "    aug_images.append(cv2.warpAffine(image, M, (w, h)))\n",
    "\n",
    "    return aug_images\n",
    "\n",
    "def process_frames_with_augmentation(image, holistic):\n",
    "   \n",
    "    keypoints = []\n",
    "\n",
    "    augmented_images = augment_image(image)\n",
    "\n",
    "    for aug_image in augmented_images:\n",
    "      \n",
    "        image, results = mediapipe_detection(aug_image, holistic)\n",
    "        keypoints.append(extract_keypoints(results))\n",
    "\n",
    "    return keypoints\n",
    "\n",
    "for action in actions:\n",
    "    action_dir = os.path.join(DATA_PATH, action)\n",
    "    available_frames = sorted([f for f in os.listdir(action_dir) if f.endswith('.jpg')])  \n",
    "\n",
    "    \n",
    "    augmented_frame_count = len(available_frames) * 6  \n",
    "\n",
    "\n",
    "    no_sequences = augmented_frame_count // sequence_length  \n",
    "\n",
    "    print(f\"Processing action: {action}, Available frames: {augmented_frame_count}, Sequences to extract: {no_sequences}\")\n",
    "\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "\n",
    "            frame_filename = available_frames[sequence * sequence_length // 6 + frame_num // 6] \n",
    "            frame_path = os.path.join(action_dir, frame_filename)\n",
    "\n",
    "            if os.path.isfile(frame_path):\n",
    "                image = cv2.imread(frame_path)\n",
    "\n",
    "                with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "                    keypoints = process_frames_with_augmentation(image, holistic)\n",
    "                    window.extend(keypoints)\n",
    "\n",
    "        if len(window) == sequence_length * 6:  \n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCcpFQUFzp1q",
    "outputId": "80fd5ae0-d7ce-416c-9360-e6535295f671"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (66, 180, 258)\n",
      "y_train shape: (66, 8)\n",
      "X_test shape: (23, 180, 258)\n",
      "y_test shape: (23, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "4hVZPiHmsliY",
    "outputId": "becb3ba0-111f-4d50-e9ba-7943450469ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">527,360</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_12               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">520</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m527,360\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_13 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m197,120\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_14 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m49,408\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_12               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m4,160\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m520\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">780,360</span> (2.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m780,360\u001b[0m (2.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">779,464</span> (2.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m779,464\u001b[0m (2.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(180, 258),\n",
    "               kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64, return_sequences=False, kernel_regularizer=l2(0.0005), recurrent_regularizer=l2(0.0005)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.0005)))\n",
    "num_classes = 8\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "optimizer = Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--Hku97N3Cq7",
    "outputId": "23d6f34a-4bbb-49b1-9bd6-0c167fd79617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 944ms/step - categorical_accuracy: 0.1213 - loss: 2.7368 - val_categorical_accuracy: 0.1304 - val_loss: 2.5718\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.2322 - loss: 2.5145 - val_categorical_accuracy: 0.1304 - val_loss: 2.5715\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 778ms/step - categorical_accuracy: 0.1992 - loss: 2.4770 - val_categorical_accuracy: 0.1304 - val_loss: 2.5714\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 854ms/step - categorical_accuracy: 0.2965 - loss: 2.2748 - val_categorical_accuracy: 0.1304 - val_loss: 2.5700\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 977ms/step - categorical_accuracy: 0.2430 - loss: 2.3990 - val_categorical_accuracy: 0.1304 - val_loss: 2.5691\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 760ms/step - categorical_accuracy: 0.2274 - loss: 2.4437 - val_categorical_accuracy: 0.1304 - val_loss: 2.5672\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 800ms/step - categorical_accuracy: 0.2713 - loss: 2.3475 - val_categorical_accuracy: 0.1304 - val_loss: 2.5656\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.1724 - loss: 2.4213 - val_categorical_accuracy: 0.1304 - val_loss: 2.5639\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 888ms/step - categorical_accuracy: 0.3033 - loss: 2.1939 - val_categorical_accuracy: 0.1304 - val_loss: 2.5624\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 940ms/step - categorical_accuracy: 0.3104 - loss: 2.3500 - val_categorical_accuracy: 0.1304 - val_loss: 2.5598\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 755ms/step - categorical_accuracy: 0.3931 - loss: 2.1104 - val_categorical_accuracy: 0.1304 - val_loss: 2.5584\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - categorical_accuracy: 0.2677 - loss: 2.2384 - val_categorical_accuracy: 0.1304 - val_loss: 2.5568\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 761ms/step - categorical_accuracy: 0.3973 - loss: 2.1342 - val_categorical_accuracy: 0.0870 - val_loss: 2.5556\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.2393 - loss: 2.2226 - val_categorical_accuracy: 0.0870 - val_loss: 2.5543\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 754ms/step - categorical_accuracy: 0.3207 - loss: 2.0654 - val_categorical_accuracy: 0.0870 - val_loss: 2.5531\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - categorical_accuracy: 0.3679 - loss: 2.1085 - val_categorical_accuracy: 0.0870 - val_loss: 2.5520\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 760ms/step - categorical_accuracy: 0.3559 - loss: 2.1391 - val_categorical_accuracy: 0.0870 - val_loss: 2.5505\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.3884 - loss: 2.0478 - val_categorical_accuracy: 0.0870 - val_loss: 2.5484\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 760ms/step - categorical_accuracy: 0.3900 - loss: 2.0122 - val_categorical_accuracy: 0.1304 - val_loss: 2.5464\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - categorical_accuracy: 0.3024 - loss: 2.1785 - val_categorical_accuracy: 0.1304 - val_loss: 2.5442\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 754ms/step - categorical_accuracy: 0.3673 - loss: 2.0600 - val_categorical_accuracy: 0.1304 - val_loss: 2.5429\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - categorical_accuracy: 0.4220 - loss: 1.9777 - val_categorical_accuracy: 0.1739 - val_loss: 2.5411\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 765ms/step - categorical_accuracy: 0.3823 - loss: 2.0222 - val_categorical_accuracy: 0.1739 - val_loss: 2.5392\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 758ms/step - categorical_accuracy: 0.4579 - loss: 1.9586 - val_categorical_accuracy: 0.2174 - val_loss: 2.5378\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.4658 - loss: 2.0406 - val_categorical_accuracy: 0.2174 - val_loss: 2.5348\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 749ms/step - categorical_accuracy: 0.4272 - loss: 2.0473 - val_categorical_accuracy: 0.1739 - val_loss: 2.5324\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.5973 - loss: 1.8997 - val_categorical_accuracy: 0.1304 - val_loss: 2.5291\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 747ms/step - categorical_accuracy: 0.4821 - loss: 2.0576 - val_categorical_accuracy: 0.1304 - val_loss: 2.5269\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 836ms/step - categorical_accuracy: 0.4851 - loss: 1.9745 - val_categorical_accuracy: 0.1304 - val_loss: 2.5240\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.5893 - loss: 1.8133 - val_categorical_accuracy: 0.1304 - val_loss: 2.5194\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 750ms/step - categorical_accuracy: 0.4833 - loss: 1.9440 - val_categorical_accuracy: 0.1304 - val_loss: 2.5155\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 756ms/step - categorical_accuracy: 0.5548 - loss: 2.0215 - val_categorical_accuracy: 0.1304 - val_loss: 2.5092\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.4932 - loss: 1.9670 - val_categorical_accuracy: 0.1304 - val_loss: 2.5023\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 780ms/step - categorical_accuracy: 0.5224 - loss: 1.8662 - val_categorical_accuracy: 0.1739 - val_loss: 2.4972\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.4474 - loss: 1.9541 - val_categorical_accuracy: 0.1739 - val_loss: 2.4909\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 790ms/step - categorical_accuracy: 0.5126 - loss: 1.8895 - val_categorical_accuracy: 0.1739 - val_loss: 2.4828\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 844ms/step - categorical_accuracy: 0.5406 - loss: 1.9080 - val_categorical_accuracy: 0.1739 - val_loss: 2.4749\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 956ms/step - categorical_accuracy: 0.5541 - loss: 1.8224 - val_categorical_accuracy: 0.1739 - val_loss: 2.4691\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 761ms/step - categorical_accuracy: 0.6421 - loss: 1.7625 - val_categorical_accuracy: 0.1739 - val_loss: 2.4589\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.5707 - loss: 1.9319 - val_categorical_accuracy: 0.1739 - val_loss: 2.4489\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 903ms/step - categorical_accuracy: 0.6067 - loss: 1.7583 - val_categorical_accuracy: 0.1739 - val_loss: 2.4417\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 749ms/step - categorical_accuracy: 0.6003 - loss: 1.7821 - val_categorical_accuracy: 0.2174 - val_loss: 2.4311\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.6265 - loss: 1.8286 - val_categorical_accuracy: 0.2609 - val_loss: 2.4185\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 874ms/step - categorical_accuracy: 0.6229 - loss: 1.7646 - val_categorical_accuracy: 0.3043 - val_loss: 2.4040\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 768ms/step - categorical_accuracy: 0.6470 - loss: 1.7505 - val_categorical_accuracy: 0.3478 - val_loss: 2.3899\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 994ms/step - categorical_accuracy: 0.4936 - loss: 1.8583 - val_categorical_accuracy: 0.3478 - val_loss: 2.3758\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 906ms/step - categorical_accuracy: 0.6067 - loss: 1.8248 - val_categorical_accuracy: 0.3043 - val_loss: 2.3637\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 792ms/step - categorical_accuracy: 0.5668 - loss: 1.7694 - val_categorical_accuracy: 0.3043 - val_loss: 2.3476\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.6244 - loss: 1.7079 - val_categorical_accuracy: 0.3043 - val_loss: 2.3349\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 869ms/step - categorical_accuracy: 0.7238 - loss: 1.6979 - val_categorical_accuracy: 0.3478 - val_loss: 2.3242\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 758ms/step - categorical_accuracy: 0.6615 - loss: 1.7869 - val_categorical_accuracy: 0.3043 - val_loss: 2.3117\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.5357 - loss: 1.8717 - val_categorical_accuracy: 0.3043 - val_loss: 2.2974\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 858ms/step - categorical_accuracy: 0.7056 - loss: 1.6305 - val_categorical_accuracy: 0.3478 - val_loss: 2.2835\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 798ms/step - categorical_accuracy: 0.7005 - loss: 1.5933 - val_categorical_accuracy: 0.3043 - val_loss: 2.2708\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.7390 - loss: 1.6502 - val_categorical_accuracy: 0.3043 - val_loss: 2.2626\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 770ms/step - categorical_accuracy: 0.6117 - loss: 1.7798 - val_categorical_accuracy: 0.3478 - val_loss: 2.2440\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - categorical_accuracy: 0.7210 - loss: 1.6852 - val_categorical_accuracy: 0.3478 - val_loss: 2.2273\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 788ms/step - categorical_accuracy: 0.6907 - loss: 1.6984 - val_categorical_accuracy: 0.4348 - val_loss: 2.2079\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - categorical_accuracy: 0.7306 - loss: 1.6585 - val_categorical_accuracy: 0.4783 - val_loss: 2.1930\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 816ms/step - categorical_accuracy: 0.6691 - loss: 1.6271 - val_categorical_accuracy: 0.4783 - val_loss: 2.1807\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 761ms/step - categorical_accuracy: 0.7894 - loss: 1.6283 - val_categorical_accuracy: 0.4783 - val_loss: 2.1686\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7306 - loss: 1.5872 - val_categorical_accuracy: 0.4783 - val_loss: 2.1546\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 767ms/step - categorical_accuracy: 0.7920 - loss: 1.5720 - val_categorical_accuracy: 0.4783 - val_loss: 2.1462\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - categorical_accuracy: 0.7936 - loss: 1.6056 - val_categorical_accuracy: 0.4348 - val_loss: 2.1365\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 769ms/step - categorical_accuracy: 0.6889 - loss: 1.6371 - val_categorical_accuracy: 0.4348 - val_loss: 2.1260\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - categorical_accuracy: 0.7270 - loss: 1.5634 - val_categorical_accuracy: 0.4348 - val_loss: 2.1145\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 829ms/step - categorical_accuracy: 0.7165 - loss: 1.6307 - val_categorical_accuracy: 0.4348 - val_loss: 2.1075\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.8307 - loss: 1.5680 - val_categorical_accuracy: 0.4348 - val_loss: 2.0979\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 780ms/step - categorical_accuracy: 0.8387 - loss: 1.5086 - val_categorical_accuracy: 0.4348 - val_loss: 2.0858\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7397 - loss: 1.5201 - val_categorical_accuracy: 0.4348 - val_loss: 2.0699\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 757ms/step - categorical_accuracy: 0.6044 - loss: 1.7943 - val_categorical_accuracy: 0.4348 - val_loss: 2.0566\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7855 - loss: 1.5275 - val_categorical_accuracy: 0.4348 - val_loss: 2.0486\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 751ms/step - categorical_accuracy: 0.6866 - loss: 1.5746 - val_categorical_accuracy: 0.4348 - val_loss: 2.0403\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7450 - loss: 1.5290 - val_categorical_accuracy: 0.4348 - val_loss: 2.0330\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 769ms/step - categorical_accuracy: 0.7848 - loss: 1.5730 - val_categorical_accuracy: 0.4783 - val_loss: 2.0240\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 848ms/step - categorical_accuracy: 0.7084 - loss: 1.5697 - val_categorical_accuracy: 0.4783 - val_loss: 2.0083\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - categorical_accuracy: 0.6860 - loss: 1.5733 - val_categorical_accuracy: 0.4348 - val_loss: 1.9934\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 762ms/step - categorical_accuracy: 0.8707 - loss: 1.4193 - val_categorical_accuracy: 0.4348 - val_loss: 1.9771\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 842ms/step - categorical_accuracy: 0.8542 - loss: 1.4722 - val_categorical_accuracy: 0.4783 - val_loss: 1.9553\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 964ms/step - categorical_accuracy: 0.8037 - loss: 1.4898 - val_categorical_accuracy: 0.4348 - val_loss: 1.9416\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 955ms/step - categorical_accuracy: 0.8217 - loss: 1.4462 - val_categorical_accuracy: 0.4348 - val_loss: 1.9297\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 924ms/step - categorical_accuracy: 0.7872 - loss: 1.5148 - val_categorical_accuracy: 0.4783 - val_loss: 1.9164\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 778ms/step - categorical_accuracy: 0.7820 - loss: 1.4755 - val_categorical_accuracy: 0.4783 - val_loss: 1.9055\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 805ms/step - categorical_accuracy: 0.8321 - loss: 1.4749 - val_categorical_accuracy: 0.4783 - val_loss: 1.8994\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 953ms/step - categorical_accuracy: 0.6766 - loss: 1.6092 - val_categorical_accuracy: 0.5217 - val_loss: 1.8956\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 907ms/step - categorical_accuracy: 0.8354 - loss: 1.5641 - val_categorical_accuracy: 0.4783 - val_loss: 1.8881\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 927ms/step - categorical_accuracy: 0.8286 - loss: 1.4919 - val_categorical_accuracy: 0.4348 - val_loss: 1.8840\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 758ms/step - categorical_accuracy: 0.8236 - loss: 1.5222 - val_categorical_accuracy: 0.4348 - val_loss: 1.8773\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 955ms/step - categorical_accuracy: 0.7702 - loss: 1.5028 - val_categorical_accuracy: 0.4348 - val_loss: 1.8634\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 962ms/step - categorical_accuracy: 0.8601 - loss: 1.4251 - val_categorical_accuracy: 0.4348 - val_loss: 1.8543\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - categorical_accuracy: 0.8339 - loss: 1.4500 - val_categorical_accuracy: 0.4783 - val_loss: 1.8381\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 877ms/step - categorical_accuracy: 0.8164 - loss: 1.3789 - val_categorical_accuracy: 0.5217 - val_loss: 1.8097\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 772ms/step - categorical_accuracy: 0.8872 - loss: 1.3440 - val_categorical_accuracy: 0.5217 - val_loss: 1.7789\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7580 - loss: 1.5642 - val_categorical_accuracy: 0.5652 - val_loss: 1.7574\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 797ms/step - categorical_accuracy: 0.7696 - loss: 1.5238 - val_categorical_accuracy: 0.5652 - val_loss: 1.7467\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - categorical_accuracy: 0.8345 - loss: 1.4431 - val_categorical_accuracy: 0.5652 - val_loss: 1.7352\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 775ms/step - categorical_accuracy: 0.8206 - loss: 1.4019 - val_categorical_accuracy: 0.6087 - val_loss: 1.7295\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7138 - loss: 1.6456 - val_categorical_accuracy: 0.5652 - val_loss: 1.7209\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 782ms/step - categorical_accuracy: 0.9230 - loss: 1.3612 - val_categorical_accuracy: 0.5217 - val_loss: 1.7199\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - categorical_accuracy: 0.7826 - loss: 1.5201 - val_categorical_accuracy: 0.5217 - val_loss: 1.7280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x798f16ba9300>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Define optimizer with lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test), class_weight=class_weights, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aiy2sQWtlQd5",
    "outputId": "e8a31283-28b2-4c15-aef2-0ff2bd27be9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8kOV-_XSaDv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
